

# TODO.  abstract out the logic-specific elements here into a pluggable architecture for producing sql

# execute a facet value count query
def facet_count(fq)
# much-simplified adaptation of standard select statement
q = fq.base_query

base_signature_statement, bind_values = base_signature_statement(fq.base_query, fq.refinements, fq.logic)
refinements_statement, bind_values    = refinements_statement(fq.base_query.model, fq.refinements, fq.logic, bind_values)

signature_expr = ["facet.signature"]
signature_expr << "base.signature"   unless base_signature_statement.empty?
signature_expr << "filter.signature" unless refinements_statement.empty?

nesting_level    = refinement_nesting_level(fq.facet, fq.refinements, fq.logic)
nesting_point    = nesting_level ? "[#{nesting_level}]" : ""

return_fields = [ "#{quote_name(fq.facet)}", "count" ]
return_types  = [ String, Integer ]
facet_fields  = [ "#{quote_name(fq.facet)}#{nesting_point}", "count(#{signature_expr.join(' & ')})" ]
extra_fields(fq) do |expr, type, index|
return_fields << "extra#{index}"
return_types  << type
facet_fields  << "#{expr} AS extra#{index}"
end

pivot_statement  = ""
pivot_conditions = []
case fq.logic[fq.facet]
when :nested
pivot_conditions << "array_length(#{quote_name(fq.facet)}, 1) = ?" if nesting_level
bind_values      << nesting_level                                  if nesting_level
when :geom
if fq.refinements[fq.facet]
pivot_statement   = "SELECT #{quote_name(fq.facet)} as pivot_key, full_geom AS pivot_geom, layer AS pivot_layer FROM #{quote_name(q.model.facet_storage_name(fq.facet))}"
pivot_conditions << "ST_Within(full_geom, pivot_geom)"
pivot_conditions << "layer = pivot_layer + 1"
pivot_conditions << "pivot_key = ?"
bind_values      << fq.refinements[fq.facet].first
else
pivot_conditions << "facet.layer = 1"
end
end

facet_conditions = []
facet_conditions << "count >= #{fq.minimum}"              if fq.minimum > 0
facet_conditions << "#{quote_name(fq.facet)} IS NOT NULL" if !fq.nullable

facet_count_order_statement = facet_count_order_statement(fq.facet, fq.order, facet_fields)

statement =  "SELECT #{return_fields.join(', ')} FROM ("
statement << "SELECT #{facet_fields.join(', ')}"
statement << " FROM #{quote_name(q.model.facet_storage_name(fq.facet))} AS facet"
statement << ", (#{base_signature_statement}) AS base"        unless base_signature_statement.empty?
statement << ", (#{refinements_statement}) AS filter"         unless refinements_statement.empty?
statement << ", (#{pivot_statement}) AS pivot"                unless pivot_statement.empty?
statement << " WHERE #{pivot_conditions.join(' AND ')}"       unless pivot_conditions.empty?
statement << ") AS facet_counts"
statement << " WHERE #{facet_conditions.join(' AND ')}"       unless facet_conditions.empty?
statement << " ORDER BY #{facet_count_order_statement}"       unless facet_count_order_statement.empty?

read_array_from_sql(return_types, statement, bind_values)
end

# Constructs SELECT statement for given query,
#
# @return [String] SELECT statement as a string
#
# @api private
def facet_results(fq)
# use base and filter as signature as subselect, then get model fields

# much-simplified adaptation of standard select statement
q       = fq.base_query
qualify = false

base_signature_statement, bind_values = base_signature_statement(q, fq.refinements, fq.logic)
refinements_statement, bind_values    = refinements_statement(q.model, fq.refinements, fq.logic, bind_values)

signature_expr = []
signature_expr << "base.signature"   unless base_signature_statement.empty?
signature_expr << "filter.signature" unless refinements_statement.empty?

faceted_expr = []
faceted_expr << "(#{base_signature_statement}) AS base"   unless base_signature_statement.empty?
faceted_expr << "(#{refinements_statement}) AS filter"    unless refinements_statement.empty?

statement = "SELECT #{columns_statement(q.fields, qualify)}"
statement << " FROM #{quote_name(q.model.storage_name(name))}"
unless signature_expr.empty?
statement << ", (SELECT members(#{signature_expr.join(' & ')}) FROM "
statement << faceted_expr.join(', ')
statement << ") AS faceted"
statement << " WHERE #{quote_name(q.model.storage_name(name))}.#{quote_name(q.model.signature_id_column)} = faceted.members"
end
statement << " ORDER BY #{order_statement(q.order, qualify)}"   if q.order && q.order.any?

add_limit_offset!(statement, q.limit, q.offset, bind_values)

read_hash_from_sql(q.fields, statement, bind_values)
end

private

def refinement_nesting_level(facet, refinements, logic)
case
when logic[facet] != :nested then nil
when refinements[facet].nil? then 1
else refinements[facet].length + 1
end
end

def facet_count_order_statement(facet, order, facet_fields)
# TODO.  make ordering more flexible; add to facet_fields
facet_sym = facet.to_sym
statement = []
order.each do |field|
statement << case field
when :count.desc then                     "count DESC"
when :count.asc, :count, 'count' then     "count ASC" 
when facet_sym.desc then                  "#{facet} DESC"
when facet_sym.asc, facet_sym, facet then "#{facet} ASC"
when :id.asc, :id, 'id' then              facet_fields << "id"; "id ASC"
else raise "Unkown order: #{order}"
end
end        
statement.join(', ')
end

def base_signature_statement(base_query, refinements, logic)
model       = base_query.model
conditions  = base_query.conditions
qualify     = false

conditions_statement, bind_values = conditions_statement(conditions, qualify)

statement = ""
unless conditions_statement.blank?
statement << "SELECT signature(#{quote_name(model.signature_id_column)}) FROM #{quote_name(model.storage_name(name))}"
statement << " WHERE #{conditions_statement}"
end

return statement, bind_values
end

def refinements_statement(model, refinements, logic, bind_values=[])
statements = []
refinements.each do |facet, values|
facet_value_conditions = facet_value_conditions(facet, values, logic)

aggregate_fn = case logic[facet]
when nil, :and           then 'filter'
when :or, :nested, :geom then 'collect'
else raise "Unkown facet refinement logic option: #{logic[facet]}"
end

expr = "SELECT #{aggregate_fn}(signature) AS signature"
expr << " FROM #{quote_name(model.facet_storage_name(facet))}"
expr << " WHERE #{facet_value_conditions}"
statements << expr
bind_values += values
end

filtered_stmt = ""
filtered_stmt << "SELECT filter(signature) AS signature FROM (#{statements.join(' UNION ')}) AS filter_elems" unless statements.empty?

return filtered_stmt, bind_values
end

def facet_value_conditions(facet, values, logic)
placeholders = values.map { '?' }
statement = case logic[facet]
when :nested then " #{quote_name(facet.to_s)} = ARRAY [ #{placeholders.join(', ')} ]"
else              " #{quote_name(facet.to_s)} IN (#{placeholders.join(', ')})"
end
return statement
end

def extra_fields(fq)
case fq.logic[fq.facet]
when :geom
yield "label", String, 2
yield "ST_AsKML(display_geom)", String, 3
yield "GeometryType(display_geom)", String, 4
yield "ST_AsKML(ST_PointOnSurface(display_geom))", String, 5
end
end

def read_hash_from_sql(fields, statement, bind_values)
types  = fields.map { |property| property.primitive }
records = []

with_connection do |connection|
command = connection.create_command(statement)
command.set_types(types)

reader = command.execute_reader(*bind_values)

begin
while reader.next!
  records << fields.zip(reader.values).to_hash
end
ensure
reader.close
end
end

records
end

def read_array_from_sql(types, statement, bind_values)
records = []

with_connection do |connection|
command = connection.create_command(statement)
command.set_types(types)

reader = command.execute_reader(*bind_values)

begin
while reader.next!
  records << reader.values
end
ensure
reader.close
end
end

records
end
end


module Repertoire
  module Faceting

    class BBox
      attr_reader :x_min, :y_min, :x_max, :y_max
      def initialize(x1, y1, x2, y2)        
        @x_min = [x1, x2].min; @x_max = [x1, x2].max
        @y_min = [y1, y2].min; @y_max = [y1, y2].max
      end
      def center
        [ (@x_max + @x_min) / 2.0, (@y_max + @y_min) / 2.0 ]
      end
      def extend(other)
        @x_min = [@x_min, other.x_min].min; @x_max = [@x_max, other.x_max].max
        @y_min = [@y_min, other.y_min].min; @y_max = [@y_max, other.y_max].max
        self
      end
      def camera_range
        ([(@x_max - @x_min).abs, (@y_max - @y_min).abs].max * 56000).floor
      end
      def to_s
        [@x_min, @x_max, @y_min, @y_max].join(', ')
      end
    end
  end
end

======= potentially of use later:

PG_FUNCTION_INFO_V1( sig_or );

Datum
sig_or( PG_FUNCTION_ARGS )
{
	Signature *sig1,  
	          *sig2,
		        *res;
	int32 sig1bytes, 
	      sig2bytes,
		    resbytes,
				i;
	uint8 c;
	
	sig1 = PG_GETARG_SIGNATURE_P(0);
	sig1bytes = VARSIZE(sig1) - VARHDRSZ - SIGNATUREHDRSZ;
	
	sig2 = PG_GETARG_SIGNATURE_P(1);
	sig2bytes = VARSIZE(sig2) - VARHDRSZ - SIGNATUREHDRSZ;
	
	resbytes = MAX(sig1bytes, sig2bytes);
	
	// if aggregate accumulator, don't allocate new memory
	if (fcinfo->context && IsA(fcinfo->context, AggState) && resbytes == sig1bytes) {
		res = sig1;
	} else {
		res = (Signature *) palloc0( resbytes + VARHDRSZ + SIGNATUREHDRSZ );
		SET_VARSIZE(res, resbytes + VARHDRSZ + SIGNATUREHDRSZ );
	}
	res->len = MAX(sig1->len, sig2->len);
	
	for(i=0; i<resbytes; i++) {
		c = 0;
		if (i < sig1bytes) {
			c |= sig1->data[i];
		}
		if (i < sig2bytes) {
			c |= sig2->data[i];
		}
		res->data[i] = c;
	}
	
	PG_FREE_IF_COPY(sig1, 0);
	PG_FREE_IF_COPY(sig2, 1);
	
	PG_RETURN_SIGNATURE_P( res );
}


-- sql to see if packed_id argument was provided and add clause

  sql = 'UPDATE ' || facet_table_name(context, facet.name)

  IF (NOT like(sql, '%WHERE%')) THEN
    sql = sql || ' WHERE _packed_id = ' || quote_literal(packed_id);
  ELSE  
    sql = sql || ' AND _packed_id = ' || quote_literal(packed_id);
  END IF;
END IF;




-- Facet declarations table

CREATE TABLE _facets(
  context TEXT NOT NULL,
  name TEXT NOT NULL,
  select_expr TEXT CHECK (select_expr IS NULL OR select_expr LIKE 'SELECT % FROM %'),
  PRIMARY KEY (context, name)
);

-- Utility functions for naming facet index tables and sequences

CREATE OR REPLACE FUNCTION facet_table_name(context TEXT, name TEXT) RETURNS TEXT AS $$
BEGIN
  RETURN quote_ident('_' || context || '_' || name || '_facet');
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION facet_seq_name(context TEXT) RETURNS TEXT AS $$
BEGIN
  RETURN quote_ident('_' || context || '_packed_id_seq');
END;
$$ LANGUAGE plpgsql;

-- Declare that a table will be used as faceting context  [ provides for packed ids ]

CREATE OR REPLACE FUNCTION declare_context(context TEXT) RETURNS VOID AS $$
BEGIN
  EXECUTE 'CREATE SEQUENCE ' || facet_seq_name(context);
  EXECUTE 'ALTER TABLE ' || quote_ident(context) || ' ADD COLUMN _packed_id INT UNIQUE DEFAULT nextval( ''' || facet_seq_name(context) || ''' )';
END;
$$ LANGUAGE plpgsql;

-- Update all facet counts for the given context

CREATE OR REPLACE FUNCTION reindex_facets(context TEXT) RETURNS VOID AS $$
DECLARE
  select_expr TEXT;
BEGIN
  -- Pack index ids
  EXECUTE 'ALTER SEQUENCE ' || facet_seq_name(context) || ' RESTART WITH 1';
  EXECUTE 'UPDATE production SET _packed_id = nextval( ''' || facet_seq_name(context) || ''' )';
  -- Update facets for context table
  FOR facet IN SELECT * FROM _facets WHERE _facets.context = context LOOP
    select_expr = facet.select_expr;
    -- From expr defaults to context table and facet column
    IF (select_expr IS NULL) THEN
      select_expr = 'SELECT ' || facet.name || ' FROM ' || facet.context;
    END IF;
    -- Augment to collect signature
    select_expr = replace(select_expr, 'FROM', ', sig_collect(' || context || '._packed_id) AS signature FROM');
	  -- Remove old facet value table
	  EXECUTE 'DROP TABLE IF EXISTS ' || facet_table_name(context, facet.name);
	  -- Create facet value table, with signature of ids
	  EXECUTE 'CREATE TABLE ' || facet_table_name(context, facet.name) || ' AS ' || select_expr 
	                          || ' GROUP BY ' || facet.name;
  END LOOP;
END;
$$ LANGUAGE plpgsql;

-- Update all facet counts for the given context and id

CREATE OR REPLACE FUNCTION reindex_facets(context TEXT, packed_id INT) RETURNS VOID AS $$
BEGIN
  -- (1) increment packed id
  -- (2) update any existing facet values
  -- (3) add any new facet values
  RAISE EXCEPTION 'Not implemented yet';
END;
$$ LANGUAGE plpgsql;



SELECT facet.region, count(base.signature & filter.signature & facet.signature) AS count 
    FROM _projects_region_facet AS facet,
  (SELECT sig_collect(_packed_id) AS signature FROM project WHERE fulltext @@ to_tsquery('Bush')) AS base,
  (SELECT sig_filter(signature) AS signature FROM
    (SELECT signature FROM _project_feature_facet WHERE feature = 'browse' UNION
     SELECT signature FROM _project_pi_facet      WHERE pi = 'Fendt')) AS filter
  WHERE count > 0 ORDER BY count DESC, facet.region ASC;


	

nobelists = YAML::load( File.open( './spec/nobelists.yml' ) )
nobelists.each do |n|
nobelist = Nobelist.create(n)
puts nobelist.name
end


    
      # TODO.  factor out common parts of facet_count and facet_result
    
      def facet_count(*args)
        query = args.last.kind_of?(Hash) ? args.pop : {}
        facet = args.first
        adapter = repository.adapter

        raise "Property #{facet} must be declared as a facet" unless self.facet?(facet)

        # note: only conditions applies to base query; all others are to facet index query
        refinements = query.delete(:refinements) || query.only(*@facets.keys)
        query.delete_if { |k, v| facet?(k) }
        minimum       = query.delete(:minimum)
        order         = query.delete(:order)    # probably NOT a property (e.g. count)
        query         = scoped_query(query)

        base   = adapter.signature(query)
        filter = filter(adapter, refinements)

        adapter.facet_count(query.model, facet, minimum, order, query.limit, query.offset , base, filter)
      end

      def facet_result(*args)
        query = args.last.kind_of?(Hash) ? args.pop : {}
        adapter = repository.adapter
      
        refinements = query.delete(:refinements) || query.only(*@facets.keys)
        query.delete_if { |k, v| facet?(k) }
        order       = query.delete(:order)
      
        base = adapter.signature(scoped_query(query))
        filter = filter(adapter, refinements)
      
        adapter.de_signature(self, base, filter, order)
      end
    
      private
      # (3) run a signature filter query with the facet columns, returning bitset as string
      def filter(adapter, refinements)
        refinements.empty? ? nil : adapter.filter(storage_name, refinements)
      end
    end



-- extra functions for hash operator class (but for some reason postgres' hash_any fn core dumps.  params not right?
-- no big advantage, since will rarely be merging more than 10 signatures with UNION

--CREATE OR REPLACE FUNCTION sig_hash( signature )
--  RETURNS int4
--  AS 'signature.so', 'sig_hash'
--  LANGUAGE C STRICT IMMUTABLE;	

--CREATE OPERATOR CLASS signature_ops
--DEFAULT FOR TYPE signature USING hash AS
--    OPERATOR    1   = ,
--    FUNCTION    1   sig_hash(signature);


/*
PG_FUNCTION_INFO_V1(sig_hash);

Datum
sig_hash(PG_FUNCTION_ARGS)
{
	Signature *sig = (Signature *) PG_GETARG_POINTER(0);
	int32     sig_bytes,
            sig_bits;
  uint8     x;
	Datum		  result;
	
  sig_bytes = sig->len / 8;
  sig_bits  = sig->len % 8;
  
  // clear unused bits to ensure hash equality
  if (sig_bits > 0) {
  	x = 0xFF >> sig_bits;
  	sig->data[sig_bytes] &= ~x;
  }

	result = hash_any((unsigned char *) sig->data, sig_bytes + 1);

	// Avoid leaking memory for toasted inputs
	PG_FREE_IF_COPY(sig, 0);

	PG_RETURN_DATUM(result);
}






// end gis facet factory
return self;
}





  // if called before google earth ready, just return
  if (!ge)
    return;
  
  // determine how feature indices map onto quantiles
  var category_size = counts.length / options.quantiles.categories;
  console.log("category size: " + category_size);
  
  // create a placemark style for each quantile
  var styles = [];
  for (var i = 0; i < options.quantiles.categories; i += 1 ) {
    var fraction = i / options.quantiles.categories;
    
    var style    = gex.dom.buildStyle(options.style || {});
    var color    = gex.util.blendColors(options.quantiles.low, options.quantiles.high, fraction);
    //style.getPolyStyle().setColor(color);
    console.log("category " + i + ": " + fraction);
    
    styles[i] = style;
  }
  
  // create a map between feature ids and their choropleth styles
  var quantile      = {};
  $.each(counts, function(index, facet_value_count) {
    var value    = facet_value_count[0];
    var count    = facet_value_count[1];
    var category = Math.floor( index / category_size );
    quantile[value] = styles[category];
  })

  // walk the dom and update style on all matching placemarks
  gex.dom.walk({
    rootObject: ge,
    visitCallback: function() {
      console.log(this.getType());
      if ('getType' in this && this.getType() == 'KmlPlacemark') {
        var id    = this.getId();
        var style = quantile[id];
        
        console.log(id);
      
        //if (style) {
          console.log('setting ' + id);
          this.setStyleSelector(style);
          this.setVisibility(true);
        //} else {
        //  console.log('hiding ' + id);
        //  this.setVisibility(false);
        //}
      }
      
      return true;
    }
  });
  
  
  
   '<ExtendedData>' +
   '<Data name="label">' +
   '<value>' + label + '</value>' +
   '</Data>' +
   '</ExtendedData>' +



   --
   -- Aggregate for generating weighted sample data from tables
   --
   -- Usage:
   --
   --    Given a table of data and frequencies, generate an array of 45 
   --      statistically-representative values:
   --
   --    SELECT weighted_sample(surname, frequency, 45) FROM male_names;
   -- 
   --    If you only want one:
   --
   --    SELECT weighted_sample(surname, frequency) FROM male_names;
   --
   --    The frequency can be any series of numbers representing relative
   --    weights.  It is not necessary that they sum to 1.0.  The values them-
   --    selves are cast to TEXT.
   --
   --    You can turn the resulting values back into rows with unnest() and
   --    join them to other sample data.  See the Postgresql 8.4 documentation.
   --

   CREATE TYPE sample AS (vals TEXT[], freqs DOUBLE PRECISION[], sum DOUBLE PRECISION, size INTEGER);

   --
   -- Given X (a series of vals) and Y (a series of DOUBLE PRECISION values), return a random
   -- X/id that conforms to the weighted sample of all values Y within the total.
   --
   CREATE OR REPLACE FUNCTION sample_matrix(state sample) RETURNS TEXT[] AS $$
   DECLARE
     running_sum DOUBLE PRECISION;
     i           INTEGER;
     rand        DOUBLE PRECISION;
     samples     TEXT[];
   BEGIN
     FOR i IN 1..state.size LOOP
       -- select a random value and loop through until hitting the corresponding item
       rand        := random();
       running_sum := 0.0;
       i           := 0;
       WHILE rand >= running_sum LOOP
         i := i + 1;
         running_sum := running_sum + (state.freqs[i] / state.sum);
       END LOOP;
       samples := samples || state.vals[i];
     END LOOP;
     RETURN samples;
   END
   $$ LANGUAGE plpgsql;

   CREATE OR REPLACE FUNCTION sample_matrix_single(state sample) RETURNS TEXT AS $$
   BEGIN
     RETURN (sample_matrix(state))[1];
   END
   $$ LANGUAGE plpgsql;

   CREATE OR REPLACE FUNCTION matrix_agg(state sample, id ANYELEMENT, val DOUBLE PRECISION, size INTEGER) RETURNS sample AS $$
   BEGIN
     state.size  := size;
     state.vals  := state.vals || id::TEXT;
     state.freqs := state.freqs || val::DOUBLE PRECISION;
     state.sum   := state.sum + val;
     RETURN state;
   END
   $$ LANGUAGE plpgsql;

   CREATE OR REPLACE FUNCTION matrix_agg_single(state sample, id ANYELEMENT, val DOUBLE PRECISION) RETURNS sample AS $$
   BEGIN
     state.size  := 1;
     state.vals  := state.vals || id::TEXT;
     state.freqs := state.freqs || val::DOUBLE PRECISION;
     state.sum   := state.sum + val;
     RETURN state;
   END
   $$ LANGUAGE plpgsql;

   CREATE AGGREGATE weighted_sample(ANYELEMENT, DOUBLE PRECISION)
   (
       sfunc = matrix_agg_single,
       stype = sample,
       finalfunc = sample_matrix_single,
       initcond = '({}, {}, 0.0, 0)'
   );

   CREATE AGGREGATE weighted_sample(ANYELEMENT, DOUBLE PRECISION, INTEGER)
   (
       sfunc = matrix_agg,
       stype = sample,
       finalfunc = sample_matrix,
       initcond = '({}, {}, 0.0, 0)'
   );
