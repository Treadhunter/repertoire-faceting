module Repertoire
  module Faceting

    class BBox
      attr_reader :x_min, :y_min, :x_max, :y_max
      def initialize(x1, y1, x2, y2)        
        @x_min = [x1, x2].min; @x_max = [x1, x2].max
        @y_min = [y1, y2].min; @y_max = [y1, y2].max
      end
      def center
        [ (@x_max + @x_min) / 2.0, (@y_max + @y_min) / 2.0 ]
      end
      def extend(other)
        @x_min = [@x_min, other.x_min].min; @x_max = [@x_max, other.x_max].max
        @y_min = [@y_min, other.y_min].min; @y_max = [@y_max, other.y_max].max
        self
      end
      def camera_range
        ([(@x_max - @x_min).abs, (@y_max - @y_min).abs].max * 56000).floor
      end
      def to_s
        [@x_min, @x_max, @y_min, @y_max].join(', ')
      end
    end
  end
end

======= potentially of use later:

PG_FUNCTION_INFO_V1( sig_or );

Datum
sig_or( PG_FUNCTION_ARGS )
{
	Signature *sig1,  
	          *sig2,
		        *res;
	int32 sig1bytes, 
	      sig2bytes,
		    resbytes,
				i;
	uint8 c;
	
	sig1 = PG_GETARG_SIGNATURE_P(0);
	sig1bytes = VARSIZE(sig1) - VARHDRSZ - SIGNATUREHDRSZ;
	
	sig2 = PG_GETARG_SIGNATURE_P(1);
	sig2bytes = VARSIZE(sig2) - VARHDRSZ - SIGNATUREHDRSZ;
	
	resbytes = MAX(sig1bytes, sig2bytes);
	
	// if aggregate accumulator, don't allocate new memory
	if (fcinfo->context && IsA(fcinfo->context, AggState) && resbytes == sig1bytes) {
		res = sig1;
	} else {
		res = (Signature *) palloc0( resbytes + VARHDRSZ + SIGNATUREHDRSZ );
		SET_VARSIZE(res, resbytes + VARHDRSZ + SIGNATUREHDRSZ );
	}
	res->len = MAX(sig1->len, sig2->len);
	
	for(i=0; i<resbytes; i++) {
		c = 0;
		if (i < sig1bytes) {
			c |= sig1->data[i];
		}
		if (i < sig2bytes) {
			c |= sig2->data[i];
		}
		res->data[i] = c;
	}
	
	PG_FREE_IF_COPY(sig1, 0);
	PG_FREE_IF_COPY(sig2, 1);
	
	PG_RETURN_SIGNATURE_P( res );
}


-- sql to see if packed_id argument was provided and add clause

  sql = 'UPDATE ' || facet_table_name(context, facet.name)

  IF (NOT like(sql, '%WHERE%')) THEN
    sql = sql || ' WHERE _packed_id = ' || quote_literal(packed_id);
  ELSE  
    sql = sql || ' AND _packed_id = ' || quote_literal(packed_id);
  END IF;
END IF;




-- Facet declarations table

CREATE TABLE _facets(
  context TEXT NOT NULL,
  name TEXT NOT NULL,
  select_expr TEXT CHECK (select_expr IS NULL OR select_expr LIKE 'SELECT % FROM %'),
  PRIMARY KEY (context, name)
);

-- Utility functions for naming facet index tables and sequences

CREATE OR REPLACE FUNCTION facet_table_name(context TEXT, name TEXT) RETURNS TEXT AS $$
BEGIN
  RETURN quote_ident('_' || context || '_' || name || '_facet');
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION facet_seq_name(context TEXT) RETURNS TEXT AS $$
BEGIN
  RETURN quote_ident('_' || context || '_packed_id_seq');
END;
$$ LANGUAGE plpgsql;

-- Declare that a table will be used as faceting context  [ provides for packed ids ]

CREATE OR REPLACE FUNCTION declare_context(context TEXT) RETURNS VOID AS $$
BEGIN
  EXECUTE 'CREATE SEQUENCE ' || facet_seq_name(context);
  EXECUTE 'ALTER TABLE ' || quote_ident(context) || ' ADD COLUMN _packed_id INT UNIQUE DEFAULT nextval( ''' || facet_seq_name(context) || ''' )';
END;
$$ LANGUAGE plpgsql;

-- Update all facet counts for the given context

CREATE OR REPLACE FUNCTION reindex_facets(context TEXT) RETURNS VOID AS $$
DECLARE
  select_expr TEXT;
BEGIN
  -- Pack index ids
  EXECUTE 'ALTER SEQUENCE ' || facet_seq_name(context) || ' RESTART WITH 1';
  EXECUTE 'UPDATE production SET _packed_id = nextval( ''' || facet_seq_name(context) || ''' )';
  -- Update facets for context table
  FOR facet IN SELECT * FROM _facets WHERE _facets.context = context LOOP
    select_expr = facet.select_expr;
    -- From expr defaults to context table and facet column
    IF (select_expr IS NULL) THEN
      select_expr = 'SELECT ' || facet.name || ' FROM ' || facet.context;
    END IF;
    -- Augment to collect signature
    select_expr = replace(select_expr, 'FROM', ', sig_collect(' || context || '._packed_id) AS signature FROM');
	  -- Remove old facet value table
	  EXECUTE 'DROP TABLE IF EXISTS ' || facet_table_name(context, facet.name);
	  -- Create facet value table, with signature of ids
	  EXECUTE 'CREATE TABLE ' || facet_table_name(context, facet.name) || ' AS ' || select_expr 
	                          || ' GROUP BY ' || facet.name;
  END LOOP;
END;
$$ LANGUAGE plpgsql;

-- Update all facet counts for the given context and id

CREATE OR REPLACE FUNCTION reindex_facets(context TEXT, packed_id INT) RETURNS VOID AS $$
BEGIN
  -- (1) increment packed id
  -- (2) update any existing facet values
  -- (3) add any new facet values
  RAISE EXCEPTION 'Not implemented yet';
END;
$$ LANGUAGE plpgsql;



SELECT facet.region, count(base.signature & filter.signature & facet.signature) AS count 
    FROM _projects_region_facet AS facet,
  (SELECT sig_collect(_packed_id) AS signature FROM project WHERE fulltext @@ to_tsquery('Bush')) AS base,
  (SELECT sig_filter(signature) AS signature FROM
    (SELECT signature FROM _project_feature_facet WHERE feature = 'browse' UNION
     SELECT signature FROM _project_pi_facet      WHERE pi = 'Fendt')) AS filter
  WHERE count > 0 ORDER BY count DESC, facet.region ASC;


	

nobelists = YAML::load( File.open( './spec/nobelists.yml' ) )
nobelists.each do |n|
nobelist = Nobelist.create(n)
puts nobelist.name
end


    
      # TODO.  factor out common parts of facet_count and facet_result
    
      def facet_count(*args)
        query = args.last.kind_of?(Hash) ? args.pop : {}
        facet = args.first
        adapter = repository.adapter

        raise "Property #{facet} must be declared as a facet" unless self.facet?(facet)

        # note: only conditions applies to base query; all others are to facet index query
        refinements = query.delete(:refinements) || query.only(*@facets.keys)
        query.delete_if { |k, v| facet?(k) }
        minimum       = query.delete(:minimum)
        order         = query.delete(:order)    # probably NOT a property (e.g. count)
        query         = scoped_query(query)

        base   = adapter.signature(query)
        filter = filter(adapter, refinements)

        adapter.facet_count(query.model, facet, minimum, order, query.limit, query.offset , base, filter)
      end

      def facet_result(*args)
        query = args.last.kind_of?(Hash) ? args.pop : {}
        adapter = repository.adapter
      
        refinements = query.delete(:refinements) || query.only(*@facets.keys)
        query.delete_if { |k, v| facet?(k) }
        order       = query.delete(:order)
      
        base = adapter.signature(scoped_query(query))
        filter = filter(adapter, refinements)
      
        adapter.de_signature(self, base, filter, order)
      end
    
      private
      # (3) run a signature filter query with the facet columns, returning bitset as string
      def filter(adapter, refinements)
        refinements.empty? ? nil : adapter.filter(storage_name, refinements)
      end
    end



-- extra functions for hash operator class (but for some reason postgres' hash_any fn core dumps.  params not right?
-- no big advantage, since will rarely be merging more than 10 signatures with UNION

--CREATE OR REPLACE FUNCTION sig_hash( signature )
--  RETURNS int4
--  AS 'signature.so', 'sig_hash'
--  LANGUAGE C STRICT IMMUTABLE;	

--CREATE OPERATOR CLASS signature_ops
--DEFAULT FOR TYPE signature USING hash AS
--    OPERATOR    1   = ,
--    FUNCTION    1   sig_hash(signature);


/*
PG_FUNCTION_INFO_V1(sig_hash);

Datum
sig_hash(PG_FUNCTION_ARGS)
{
	Signature *sig = (Signature *) PG_GETARG_POINTER(0);
	int32     sig_bytes,
            sig_bits;
  uint8     x;
	Datum		  result;
	
  sig_bytes = sig->len / 8;
  sig_bits  = sig->len % 8;
  
  // clear unused bits to ensure hash equality
  if (sig_bits > 0) {
  	x = 0xFF >> sig_bits;
  	sig->data[sig_bytes] &= ~x;
  }

	result = hash_any((unsigned char *) sig->data, sig_bytes + 1);

	// Avoid leaking memory for toasted inputs
	PG_FREE_IF_COPY(sig, 0);

	PG_RETURN_DATUM(result);
}






// end gis facet factory
return self;
}





  // if called before google earth ready, just return
  if (!ge)
    return;
  
  // determine how feature indices map onto quantiles
  var category_size = counts.length / options.quantiles.categories;
  console.log("category size: " + category_size);
  
  // create a placemark style for each quantile
  var styles = [];
  for (var i = 0; i < options.quantiles.categories; i += 1 ) {
    var fraction = i / options.quantiles.categories;
    
    var style    = gex.dom.buildStyle(options.style || {});
    var color    = gex.util.blendColors(options.quantiles.low, options.quantiles.high, fraction);
    //style.getPolyStyle().setColor(color);
    console.log("category " + i + ": " + fraction);
    
    styles[i] = style;
  }
  
  // create a map between feature ids and their choropleth styles
  var quantile      = {};
  $.each(counts, function(index, facet_value_count) {
    var value    = facet_value_count[0];
    var count    = facet_value_count[1];
    var category = Math.floor( index / category_size );
    quantile[value] = styles[category];
  })

  // walk the dom and update style on all matching placemarks
  gex.dom.walk({
    rootObject: ge,
    visitCallback: function() {
      console.log(this.getType());
      if ('getType' in this && this.getType() == 'KmlPlacemark') {
        var id    = this.getId();
        var style = quantile[id];
        
        console.log(id);
      
        //if (style) {
          console.log('setting ' + id);
          this.setStyleSelector(style);
          this.setVisibility(true);
        //} else {
        //  console.log('hiding ' + id);
        //  this.setVisibility(false);
        //}
      }
      
      return true;
    }
  });
  
  
  
   '<ExtendedData>' +
   '<Data name="label">' +
   '<value>' + label + '</value>' +
   '</Data>' +
   '</ExtendedData>' +



   --
   -- Aggregate for generating weighted sample data from tables
   --
   -- Usage:
   --
   --    Given a table of data and frequencies, generate an array of 45 
   --      statistically-representative values:
   --
   --    SELECT weighted_sample(surname, frequency, 45) FROM male_names;
   -- 
   --    If you only want one:
   --
   --    SELECT weighted_sample(surname, frequency) FROM male_names;
   --
   --    The frequency can be any series of numbers representing relative
   --    weights.  It is not necessary that they sum to 1.0.  The values them-
   --    selves are cast to TEXT.
   --
   --    You can turn the resulting values back into rows with unnest() and
   --    join them to other sample data.  See the Postgresql 8.4 documentation.
   --

   CREATE TYPE sample AS (vals TEXT[], freqs DOUBLE PRECISION[], sum DOUBLE PRECISION, size INTEGER);

   --
   -- Given X (a series of vals) and Y (a series of DOUBLE PRECISION values), return a random
   -- X/id that conforms to the weighted sample of all values Y within the total.
   --
   CREATE OR REPLACE FUNCTION sample_matrix(state sample) RETURNS TEXT[] AS $$
   DECLARE
     running_sum DOUBLE PRECISION;
     i           INTEGER;
     rand        DOUBLE PRECISION;
     samples     TEXT[];
   BEGIN
     FOR i IN 1..state.size LOOP
       -- select a random value and loop through until hitting the corresponding item
       rand        := random();
       running_sum := 0.0;
       i           := 0;
       WHILE rand >= running_sum LOOP
         i := i + 1;
         running_sum := running_sum + (state.freqs[i] / state.sum);
       END LOOP;
       samples := samples || state.vals[i];
     END LOOP;
     RETURN samples;
   END
   $$ LANGUAGE plpgsql;

   CREATE OR REPLACE FUNCTION sample_matrix_single(state sample) RETURNS TEXT AS $$
   BEGIN
     RETURN (sample_matrix(state))[1];
   END
   $$ LANGUAGE plpgsql;

   CREATE OR REPLACE FUNCTION matrix_agg(state sample, id ANYELEMENT, val DOUBLE PRECISION, size INTEGER) RETURNS sample AS $$
   BEGIN
     state.size  := size;
     state.vals  := state.vals || id::TEXT;
     state.freqs := state.freqs || val::DOUBLE PRECISION;
     state.sum   := state.sum + val;
     RETURN state;
   END
   $$ LANGUAGE plpgsql;

   CREATE OR REPLACE FUNCTION matrix_agg_single(state sample, id ANYELEMENT, val DOUBLE PRECISION) RETURNS sample AS $$
   BEGIN
     state.size  := 1;
     state.vals  := state.vals || id::TEXT;
     state.freqs := state.freqs || val::DOUBLE PRECISION;
     state.sum   := state.sum + val;
     RETURN state;
   END
   $$ LANGUAGE plpgsql;

   CREATE AGGREGATE weighted_sample(ANYELEMENT, DOUBLE PRECISION)
   (
       sfunc = matrix_agg_single,
       stype = sample,
       finalfunc = sample_matrix_single,
       initcond = '({}, {}, 0.0, 0)'
   );

   CREATE AGGREGATE weighted_sample(ANYELEMENT, DOUBLE PRECISION, INTEGER)
   (
       sfunc = matrix_agg,
       stype = sample,
       finalfunc = sample_matrix,
       initcond = '({}, {}, 0.0, 0)'
   );
